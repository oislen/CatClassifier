{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9219296-311e-4135-8a3a-8c4320624286",
   "metadata": {},
   "source": [
    "# Cats vs Dogs Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6424bfa9-adfd-48bc-a203-b6242008dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import cons\n",
    "\n",
    "import torch\n",
    "from model.torch.VGG16_pretrained import VGG16_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05783f63-8fb2-47d0-b88a-8fc91842bd90",
   "metadata": {},
   "source": [
    "This project aims to create a model to classify cat and dog images. The data was sourced from the [dogs-vs-cats](https://www.kaggle.com/competitions/dogs-vs-cats/overview) Kaggle competition, and also from [freeimages.com](https://www.freeimages.com/) using a web scraper. Docker containers were used to deploy the application on an EC2 spot instances in order to scale up hardware and computation power. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c739375-4438-4ee1-8889-3bec76b070a7",
   "metadata": {},
   "source": [
    "## Example Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9d44483-13ac-4013-8d3e-1573872f005a",
   "metadata": {},
   "source": [
    "![Random Image](../report/torch/random_image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a586a0-abe8-4e87-8125-3a0761ecac49",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ec205-434c-4fe5-a4f0-26f811f25761",
   "metadata": {},
   "source": [
    "The images were resized to a uniform dimension prior to the modelling training phase. See example image processing below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d53af71-c2ee-4ed1-8335-5565dce7951a",
   "metadata": {},
   "source": [
    "![Generator Plot](../report/torch/generator_plot.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac34849-f97f-4086-99ca-c8ab2ba5d77e",
   "metadata": {},
   "source": [
    "## VGG16 Model Archecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1dcff-9319-4973-9659-fda87cb85481",
   "metadata": {},
   "source": [
    "A pretrained VGG CNN model with 16 layers was trained using the processed images via pytorch. See VGG16 diagram below, as well as torch model summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e6c9c-c63f-41c2-90f0-2c1321881b81",
   "metadata": {},
   "source": [
    "![AlexNet Architecture](../report/torch/VGG16_archecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f790b84-1255-42fb-b07c-37065ab49c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16_pretrained(\n",
      "  (resnet): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# load trained torch model\n",
    "model = VGG16_pretrained(num_classes=2).to(device)\n",
    "model.load(input_fpath=cons.torch_model_pt_fpath)\n",
    "# print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cdf44-b643-400e-8f14-556384ba9ad0",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771965b-b74f-4363-aa95-242038dcd235",
   "metadata": {},
   "source": [
    "The model was trained across 4 epochs. The model accuracy and loss are plotted below across the training and validation sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52070b1f-4162-4bb8-a082-534a7b858335",
   "metadata": {},
   "source": [
    "![Model Accuaracy](../report/torch/model_accuracy.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9d58fe1-fce7-4754-a35a-10a7325b7bdd",
   "metadata": {},
   "source": [
    "![Model Loss](../report/torch/model_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e17f5-70dc-4542-8c24-83c52bcc3859",
   "metadata": {},
   "source": [
    "## Model Image Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f307af91-0b9f-486f-bc8d-f8acb3845949",
   "metadata": {},
   "source": [
    "The model predictions were made for the Kaggle test set, see below example model predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8856811-a69b-4f4c-a2b6-9d528fcfb75e",
   "metadata": {},
   "source": [
    "![Predicted Images](../report/torch/pred_images.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25817b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
