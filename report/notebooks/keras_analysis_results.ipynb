{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9219296-311e-4135-8a3a-8c4320624286",
   "metadata": {},
   "source": [
    "# Cats vs Dogs Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6424bfa9-adfd-48bc-a203-b6242008dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05783f63-8fb2-47d0-b88a-8fc91842bd90",
   "metadata": {},
   "source": [
    "This project aims to create a model to classify cat and dog images. The data was sourced from the [dogs-vs-cats](https://www.kaggle.com/competitions/dogs-vs-cats/overview) Kaggle competition, and also from [freeimages.com](https://www.freeimages.com/) using a web scraper. Docker containers were used to deploy the application on an EC2 spot instances in order to scale up hardware and computation power. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c739375-4438-4ee1-8889-3bec76b070a7",
   "metadata": {},
   "source": [
    "## Example Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9d44483-13ac-4013-8d3e-1573872f005a",
   "metadata": {},
   "source": [
    "![Random Image](../report/keras/random_image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a586a0-abe8-4e87-8125-3a0761ecac49",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ec205-434c-4fe5-a4f0-26f811f25761",
   "metadata": {},
   "source": [
    "The images were further processed using rotations, scaling, zooming, flipping and shearing prior to the modelling training phase. See example image processing below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d53af71-c2ee-4ed1-8335-5565dce7951a",
   "metadata": {},
   "source": [
    "![Generator Plot](../report/keras/generator_plot.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac34849-f97f-4086-99ca-c8ab2ba5d77e",
   "metadata": {},
   "source": [
    "## AlexNet8 Model Archecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1dcff-9319-4973-9659-fda87cb85481",
   "metadata": {},
   "source": [
    "An AlexNet CNN model with 8 layers was trained using the processed images via Keras. See AlexNet diagram below, as well as keras model summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e6c9c-c63f-41c2-90f0-2c1321881b81",
   "metadata": {},
   "source": [
    "![AlexNet Architecture](../report/keras/AlexNet8_archecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f790b84-1255-42fb-b07c-37065ab49c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AlexNet8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 30, 30, 96)        34944     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 256)       614656    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 6, 384)         885120    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 384)         1327488   \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 6, 6, 256)         884992    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 2, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              4198400   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,735,106\n",
      "Trainable params: 24,735,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load trained keras model\n",
    "model = keras.models.load_model('../data/keras_model.h5')\n",
    "# print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cdf44-b643-400e-8f14-556384ba9ad0",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771965b-b74f-4363-aa95-242038dcd235",
   "metadata": {},
   "source": [
    "The model was trained across 25 epochs. The model accuracy and loss are plotted below across the training and validation sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52070b1f-4162-4bb8-a082-534a7b858335",
   "metadata": {},
   "source": [
    "![Model Accuaracy](../report/keras/model_accuracy.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9d58fe1-fce7-4754-a35a-10a7325b7bdd",
   "metadata": {},
   "source": [
    "![Model Loss](../report/keras/model_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e17f5-70dc-4542-8c24-83c52bcc3859",
   "metadata": {},
   "source": [
    "## Model Image Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f307af91-0b9f-486f-bc8d-f8acb3845949",
   "metadata": {},
   "source": [
    "The model predictions were made for the Kaggle test set, see below example model predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8856811-a69b-4f4c-a2b6-9d528fcfb75e",
   "metadata": {},
   "source": [
    "![Predicted Images](../report/keras/pred_images.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25817b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
